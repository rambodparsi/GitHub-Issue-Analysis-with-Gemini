{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e7d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo root: /Users/rambodparsi/Desktop/OSS Repository Selection/web_scrapper/commit_pr_issue_analysis\n",
      "Bootstrap ready ✓\n"
     ]
    }
   ],
   "source": [
    "# === Bootstrap cell: shared config, paths, JSON I/O, and GitHub REST/GraphQL helpers ===\n",
    "\n",
    "import os, json, time, re, random, pathlib\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Locate repo root reliably (works no matter where you open the .ipynb)\n",
    "# -------------------------------------------------------------------\n",
    "def _find_repo_root(start: pathlib.Path) -> pathlib.Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(8):  # walk up to 8 levels\n",
    "        if (cur / \"config\" / \"config.yaml\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"Couldn't locate repo root (no config/config.yaml found upward).\")\n",
    "\n",
    "REPO_ROOT = _find_repo_root(pathlib.Path.cwd())\n",
    "CONFIG_PATH = REPO_ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load config + token\n",
    "# -------------------------------------------------------------------\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG: Dict[str, Any] = yaml.safe_load(f) or {}\n",
    "\n",
    "load_dotenv(REPO_ROOT / \".env\")  # local only; do not commit .env\n",
    "TOKEN_ENV = (CFG.get(\"github_token_env\") or \"GITHUB_TOKEN\").strip()\n",
    "GITHUB_TOKEN = os.getenv(TOKEN_ENV, \"\").strip()\n",
    "if not GITHUB_TOKEN:\n",
    "    raise RuntimeError(f\"Missing token in environment variable {TOKEN_ENV}. \"\n",
    "                       f\"Create .env from .env.example and set {TOKEN_ENV}=...\")\n",
    "\n",
    "# Repo + output root\n",
    "REPO = CFG[\"repo\"]  # \"owner/name\"\n",
    "OWNER, NAME = REPO.split(\"/\", 1)\n",
    "OUT_ROOT = pathlib.Path(CFG.get(\"out_root\") or NAME)  # \"<name>\" if null\n",
    "\n",
    "# Behavior\n",
    "OVERWRITE = bool(CFG.get(\"overwrite\", True))\n",
    "VERBOSE = bool(CFG.get(\"verbose_logs\", True))\n",
    "\n",
    "# Networking knobs\n",
    "REQ_TIMEOUT = int(CFG.get(\"request_timeout_sec\", 30))\n",
    "MAX_RETRIES = int(CFG.get(\"max_retries\", 4))\n",
    "BACKOFF_BASE_MS = int(CFG.get(\"backoff_base_ms\", 400))\n",
    "BACKOFF_JITTER_MS = int(CFG.get(\"backoff_jitter_ms\", 250))\n",
    "RESPECT_RL = bool(CFG.get(\"respect_rate_limits\", True))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Tiny logging helpers\n",
    "# -------------------------------------------------------------------\n",
    "def log(msg: str) -> None:\n",
    "    if VERBOSE:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def warn(msg: str) -> None:\n",
    "    print(f\"⚠️  {msg}\", flush=True)\n",
    "\n",
    "print(\"Using repo root:\", REPO_ROOT)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Canonical output layout helpers\n",
    "# -------------------------------------------------------------------\n",
    "def ensure_dir(p: pathlib.Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def repo_root() -> pathlib.Path:\n",
    "    root = OUT_ROOT\n",
    "    ensure_dir(root)\n",
    "    return root\n",
    "\n",
    "def tags_all_json() -> pathlib.Path:\n",
    "    d = repo_root() / \"tags\"\n",
    "    ensure_dir(d)\n",
    "    return d / \"tags.all.json\"\n",
    "\n",
    "def series_dir(kind: str, series: str) -> pathlib.Path:\n",
    "    d = repo_root() / kind / series\n",
    "    ensure_dir(d)\n",
    "    return d\n",
    "\n",
    "def pair_stem(base: str, compare: str) -> str:\n",
    "    return f\"{base}...{compare}\"\n",
    "\n",
    "def pair_json(series: str, stem: str, kind: str) -> pathlib.Path:\n",
    "    # kind ∈ {\"compare\",\"commits\",\"pulls\",\"issues\"}\n",
    "    return series_dir(kind, series) / f\"{stem}.{kind}.json\"\n",
    "\n",
    "def capsule_json(series: str, stem: str) -> pathlib.Path:\n",
    "    return series_dir(\"commits_pr_issue\", series) / f\"{stem}.tarce_artifacts.json\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# JSON I/O\n",
    "# -------------------------------------------------------------------\n",
    "def read_json(path: pathlib.Path) -> Optional[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(path: pathlib.Path, payload: Dict[str, Any]) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    if path.exists() and not OVERWRITE:\n",
    "        log(f\"Skip (exists): {path}\")\n",
    "        return\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    tmp.replace(path)\n",
    "    log(f\"✓ Wrote {path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GitHub HTTP session + rate-limit aware helpers\n",
    "# -------------------------------------------------------------------\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    \"User-Agent\": f\"notebook-pipeline/{NAME}\"\n",
    "})\n",
    "\n",
    "def _parse_reset_epoch(headers: Dict[str, Any]) -> Optional[int]:\n",
    "    try:\n",
    "        return int(headers.get(\"X-RateLimit-Reset\") or headers.get(\"x-ratelimit-reset\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _maybe_sleep_for_reset(resp: requests.Response) -> None:\n",
    "    if not RESPECT_RL:\n",
    "        return\n",
    "    remaining = resp.headers.get(\"X-RateLimit-Remaining\") or resp.headers.get(\"x-ratelimit-remaining\")\n",
    "    if remaining is not None and str(remaining).isdigit() and int(remaining) <= 0:\n",
    "        reset_epoch = _parse_reset_epoch(resp.headers)\n",
    "        if reset_epoch:\n",
    "            now = int(time.time())\n",
    "            delta = max(0, reset_epoch - now) + 1\n",
    "            warn(f\"Rate limit reached. Sleeping ~{delta}s until reset …\")\n",
    "            time.sleep(delta)\n",
    "\n",
    "def _backoff_sleep(i: int) -> None:\n",
    "    base = BACKOFF_BASE_MS / 1000.0\n",
    "    jitter = random.uniform(0, BACKOFF_JITTER_MS / 1000.0)\n",
    "    time.sleep((2 ** i) * base + jitter)\n",
    "\n",
    "def rest_get_json(url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generic GET with backoff + rate-limit handling.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for i in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = SESSION.get(url, params=params, timeout=REQ_TIMEOUT)\n",
    "            # Honor 429 Retry-After if present\n",
    "            if r.status_code == 429:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                if ra and ra.isdigit():\n",
    "                    wait_s = int(ra)\n",
    "                    warn(f\"429 received. Sleeping {wait_s}s per Retry-After …\")\n",
    "                    time.sleep(wait_s)\n",
    "                    continue\n",
    "            if r.status_code >= 500:\n",
    "                last_err = f\"{r.status_code} {r.text[:200]}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"HTTP {r.status_code}: {r.text[:500]}\")\n",
    "            _maybe_sleep_for_reset(r)\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            _backoff_sleep(i)\n",
    "    raise RuntimeError(f\"GET failed after retries: {url} :: {last_err}\")\n",
    "\n",
    "def gh_graphql(query: str, variables: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GitHub GraphQL POST with backoff + rate-limit handling.\n",
    "    Endpoint: https://api.github.com/graphql\n",
    "    \"\"\"\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    payload = {\"query\": query, \"variables\": variables or {}}\n",
    "    last_err = None\n",
    "    for i in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = SESSION.post(url, json=payload, timeout=REQ_TIMEOUT)\n",
    "            if r.status_code == 429:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                if ra and ra.isdigit():\n",
    "                    wait_s = int(ra)\n",
    "                    warn(f\"429 received. Sleeping {wait_s}s per Retry-After …\")\n",
    "                    time.sleep(wait_s)\n",
    "                    continue\n",
    "            if r.status_code >= 500:\n",
    "                last_err = f\"{r.status_code} {r.text[:200]}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"GraphQL HTTP {r.status_code}: {r.text[:500]}\")\n",
    "            _maybe_sleep_for_reset(r)\n",
    "            data = r.json()\n",
    "            if \"errors\" in data:\n",
    "                last_err = f\"GraphQL errors: {data['errors']}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            _backoff_sleep(i)\n",
    "    raise RuntimeError(f\"GraphQL failed after retries: {last_err}\")\n",
    "\n",
    "def rate_limit_snapshot() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns current REST rate-limit bucket (printed; not written to JSON outputs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info = rest_get_json(\"https://api.github.com/rate_limit\")\n",
    "        core = info.get(\"resources\", {}).get(\"core\", {})\n",
    "        remaining = core.get(\"remaining\")\n",
    "        limit = core.get(\"limit\")\n",
    "        reset = core.get(\"reset\")\n",
    "        when = datetime.fromtimestamp(reset, tz=timezone.utc).isoformat() if reset else None\n",
    "        snap = {\"limit\": limit, \"remaining\": remaining, \"reset_epoch\": reset, \"reset_iso\": when}\n",
    "        log(f\"Rate limit: {remaining}/{limit}, resets at {when}\")\n",
    "        return snap\n",
    "    except Exception as e:\n",
    "        warn(f\"Rate limit snapshot failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Light helpers for series + ordering\n",
    "# -------------------------------------------------------------------\n",
    "def semver_series(tag_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract 'vX.Y' series from tags like 'v4.2.3', '4.2.0-rc.1', etc.\n",
    "    Falls back to 'v0.0' if not parseable.\n",
    "    \"\"\"\n",
    "    m = re.search(r'v?(\\d+)\\.(\\d+)', tag_name or \"\")\n",
    "    if not m:\n",
    "        return \"v0.0\"\n",
    "    return f\"v{int(m.group(1))}.{int(m.group(2))}\"\n",
    "\n",
    "def sorted_pairs_by_tag_time(pairs: List[Tuple[str, str]], tag_index: Dict[str, Dict[str, Any]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Sort (base, compare) by tag timestamps ascending when both are tags;\n",
    "    unknown timestamps sort last.\n",
    "    \"\"\"\n",
    "    def ts(tag: str) -> float:\n",
    "        rec = tag_index.get(tag)\n",
    "        if rec and rec.get(\"tag_timestamp\"):\n",
    "            try:\n",
    "                return datetime.fromisoformat(rec[\"tag_timestamp\"].replace(\"Z\", \"+00:00\")).timestamp()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return float(\"inf\")\n",
    "    return sorted(pairs, key=lambda bc: (ts(bc[0]), ts(bc[1])))\n",
    "\n",
    "log(\"Bootstrap ready ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd311f3",
   "metadata": {},
   "source": [
    "### Config echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5e12da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: mastodon/mastodon\n",
      "Output root: mastodon\n",
      "Pairs mode: series\n",
      "Series whitelist: ['v4.0']\n",
      "Explicit pairs: []\n",
      "Optional branch: None\n",
      "Rate limit: 5000/5000, resets at 2025-10-26T14:16:07+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'limit': 5000,\n",
       " 'remaining': 5000,\n",
       " 'reset_epoch': 1761488167,\n",
       " 'reset_iso': '2025-10-26T14:16:07+00:00'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Repo:\", REPO)\n",
    "print(\"Output root:\", repo_root())\n",
    "print(\"Pairs mode:\", CFG.get(\"pairs_mode\"))\n",
    "print(\"Series whitelist:\", CFG.get(\"series_whitelist\"))\n",
    "print(\"Explicit pairs:\", CFG.get(\"explicit_pairs\"))\n",
    "print(\"Optional branch:\", CFG.get(\"optional_branch\"))\n",
    "rate_limit_snapshot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc3c8c",
   "metadata": {},
   "source": [
    "### Load tags and build a tag index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "299a5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series discovered: ['v0.1', 'v0.6', 'v0.7', 'v0.8', 'v0.9', 'v1.0', 'v1.1', 'v1.2', 'v1.3', 'v1.4', 'v1.5', 'v1.6', 'v2.0', 'v2.1', 'v2.2', 'v2.3', 'v2.4', 'v2.5', 'v2.6', 'v2.7', 'v2.8', 'v2.9', 'v3.0', 'v3.1', 'v3.2', 'v3.3', 'v3.4', 'v3.5', 'v4.0', 'v4.1', 'v4.2', 'v4.3', 'v4.4', 'v4.5']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Load tags\n",
    "tags_payload = read_json(tags_all_json())\n",
    "if not tags_payload or not tags_payload.get(\"tags\"):\n",
    "    raise RuntimeError(\"tags.all.json not found or empty. Run m1_fetch_tags.ipynb first.\")\n",
    "\n",
    "all_tags = tags_payload[\"tags\"]\n",
    "\n",
    "# Index by tag name\n",
    "tag_by_name = {t[\"name\"]: t for t in all_tags}\n",
    "\n",
    "# Group tags by series and sort by timestamp (old → new)\n",
    "series_tags = defaultdict(list)\n",
    "for t in all_tags:\n",
    "    series_tags[t[\"series\"]].append(t)\n",
    "\n",
    "def ts_key(x):\n",
    "    ts = x.get(\"tag_timestamp\")\n",
    "    if not ts:\n",
    "        return float(\"inf\")\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\",\"+00:00\")).timestamp()\n",
    "    except Exception:\n",
    "        return float(\"inf\")\n",
    "\n",
    "for s in series_tags:\n",
    "    series_tags[s].sort(key=ts_key)\n",
    "\n",
    "print(\"Series discovered:\", sorted(series_tags.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca6524",
   "metadata": {},
   "source": [
    "### Build pairs according to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce2b67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned pairs: 19\n",
      "{'base': 'v4.0.0rc1', 'compare': 'v4.0.0rc2', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.0rc2', 'compare': 'v4.0.0rc3', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.0rc3', 'compare': 'v4.0.0rc4', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.0rc4', 'compare': 'v4.0.0', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.0', 'compare': 'v4.0.1', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.1', 'compare': 'v4.0.2', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.2', 'compare': 'v4.0.3', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.3', 'compare': 'v4.0.4', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.4', 'compare': 'v4.0.5', 'series': 'v4.0'}\n",
      "{'base': 'v4.0.5', 'compare': 'v4.0.6', 'series': 'v4.0'}\n",
      "…\n"
     ]
    }
   ],
   "source": [
    "PAIRS_MODE = (CFG.get(\"pairs_mode\") or \"series\").lower()\n",
    "SERIES_WHITELIST = CFG.get(\"series_whitelist\") or []\n",
    "EXPLICIT_PAIRS = CFG.get(\"explicit_pairs\") or []\n",
    "OPTIONAL_BRANCH = CFG.get(\"optional_branch\")  # e.g., \"stable-4.2\"\n",
    "\n",
    "def adjacent_pairs_for_series(series_name: str):\n",
    "    tags = series_tags.get(series_name, [])\n",
    "    names = [t[\"name\"] for t in tags]\n",
    "    pairs = []\n",
    "    for i in range(len(names)-1):\n",
    "        pairs.append({\"base\": names[i], \"compare\": names[i+1], \"series\": series_name})\n",
    "    # Append lastTag → OPTIONAL_BRANCH if configured\n",
    "    if OPTIONAL_BRANCH:\n",
    "        if names:\n",
    "            pairs.append({\"base\": names[-1], \"compare\": OPTIONAL_BRANCH, \"series\": series_name})\n",
    "    return pairs\n",
    "\n",
    "pairs_planned = []\n",
    "\n",
    "if PAIRS_MODE == \"all\":\n",
    "    for s in sorted(series_tags.keys()):\n",
    "        pairs_planned.extend(adjacent_pairs_for_series(s))\n",
    "\n",
    "elif PAIRS_MODE == \"series\":\n",
    "    target = SERIES_WHITELIST if SERIES_WHITELIST else sorted(series_tags.keys())\n",
    "    for s in target:\n",
    "        pairs_planned.extend(adjacent_pairs_for_series(s))\n",
    "\n",
    "elif PAIRS_MODE == \"explicit\":\n",
    "    # Keep series inferred from tag_by_name to place files under the correct folder.\n",
    "    for p in EXPLICIT_PAIRS:\n",
    "        base = p[\"base\"]; cmp_ = p[\"compare\"]\n",
    "        # Try to infer series from base tag; fall back to compare; else 'v0.0'\n",
    "        series_guess = \"v0.0\"\n",
    "        if base in tag_by_name:\n",
    "            series_guess = tag_by_name[base][\"series\"]\n",
    "        elif cmp_ in tag_by_name:\n",
    "            series_guess = tag_by_name[cmp_][\"series\"]\n",
    "        pairs_planned.append({\"base\": base, \"compare\": cmp_, \"series\": series_guess})\n",
    "    # OPTIONAL_BRANCH is typically not used with explicit, so we skip it here on purpose.\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(f\"Unsupported pairs_mode: {PAIRS_MODE}\")\n",
    "\n",
    "print(f\"Planned pairs: {len(pairs_planned)}\")\n",
    "for pp in pairs_planned[:10]:\n",
    "    print(pp)\n",
    "if len(pairs_planned) > 10:\n",
    "    print(\"…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89004455",
   "metadata": {},
   "source": [
    "### Compare helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "786a8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_for_compare(name_or_ref: str) -> str:\n",
    "    \"\"\"\n",
    "    If this ref matches a known tag name, prefix 'tags/' to force GitHub to\n",
    "    treat it as a tag when comparing (avoids the branch-vs-tag ambiguity).\n",
    "    Otherwise, return it unchanged (branch/SHA).\n",
    "    \"\"\"\n",
    "    if name_or_ref in tag_by_name:\n",
    "        return f\"tags/{name_or_ref}\"\n",
    "    return name_or_ref\n",
    "\n",
    "def compare_url(owner: str, repo: str, base_ref: str, head_ref: str) -> str:\n",
    "    return f\"https://api.github.com/repos/{owner}/{repo}/compare/{base_ref}...{head_ref}\"\n",
    "\n",
    "def to_commit_row(c: dict) -> dict:\n",
    "    # Map GitHub compare commit object → our flat row\n",
    "    sha = c.get(\"sha\")\n",
    "    html_url = c.get(\"html_url\") or f\"https://github.com/{OWNER}/{NAME}/commit/{sha}\" if sha else None\n",
    "    api_url = c.get(\"url\")\n",
    "    author_login = (c.get(\"author\") or {}).get(\"login\")  # may be None if author not linked\n",
    "    meta = c.get(\"commit\") or {}\n",
    "    author_meta = (meta.get(\"author\") or {})\n",
    "    commit_author_date = author_meta.get(\"date\")\n",
    "    return {\n",
    "        \"sha\": sha,\n",
    "        \"html_url\": html_url,\n",
    "        \"api_url\": api_url,\n",
    "        \"author_login\": author_login,\n",
    "        \"commit_author_date\": commit_author_date\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85434a62",
   "metadata": {},
   "source": [
    "### Run compares and write <base...compare>.compare.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d36d829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote mastodon/compare/v4.0/v4.0.0rc1...v4.0.0rc2.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.0rc2...v4.0.0rc3.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.0rc3...v4.0.0rc4.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.0rc4...v4.0.0.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.0...v4.0.1.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.1...v4.0.2.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.2...v4.0.3.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.3...v4.0.4.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.4...v4.0.5.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.5...v4.0.6.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.6...v4.0.7.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.7...v4.0.8.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.8...v4.0.9.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.9...v4.0.10.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.10...v4.0.11.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.11...v4.0.12.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.12...v4.0.13.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.13...v4.0.14.compare.json\n",
      "✓ Wrote mastodon/compare/v4.0/v4.0.14...v4.0.15.compare.json\n",
      "Done. Written: 19 | Skipped: 0 | Errors (minimal outputs created): 0\n",
      "Rate limit: 4981/5000, resets at 2025-10-26T14:16:07+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'limit': 5000,\n",
       " 'remaining': 4981,\n",
       " 'reset_epoch': 1761488167,\n",
       " 'reset_iso': '2025-10-26T14:16:07+00:00'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "written = 0\n",
    "skipped = 0\n",
    "errors = 0\n",
    "\n",
    "for pair in pairs_planned:\n",
    "    base_raw = pair[\"base\"]\n",
    "    head_raw = pair[\"compare\"]\n",
    "    series = pair[\"series\"]\n",
    "\n",
    "    base = ref_for_compare(base_raw)\n",
    "    head = ref_for_compare(head_raw)\n",
    "\n",
    "    stem = pair_stem(base_raw, head_raw)  # filename uses raw names\n",
    "    out_path = pair_json(series, stem, \"compare\")\n",
    "\n",
    "    # Skip if exists and not overwriting\n",
    "    if out_path.exists() and not OVERWRITE:\n",
    "        log(f\"Skip (exists): {out_path}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    url = compare_url(OWNER, NAME, base, head)\n",
    "    try:\n",
    "        data = rest_get_json(url)\n",
    "        commits_api = data.get(\"commits\") or []\n",
    "        commits_rows = [to_commit_row(c) for c in commits_api]\n",
    "\n",
    "        payload = {\n",
    "            \"repo\": REPO,\n",
    "            \"base\": base_raw,\n",
    "            \"compare\": head_raw,\n",
    "            \"commit_count\": len(commits_rows),\n",
    "            \"commits\": commits_rows\n",
    "        }\n",
    "        write_json(out_path, payload)\n",
    "        written += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        warn(f\"Compare failed for {base_raw}...{head_raw}: {e}\")\n",
    "        # Still emit a minimal file to keep the pipeline consistent\n",
    "        payload = {\n",
    "            \"repo\": REPO,\n",
    "            \"base\": base_raw,\n",
    "            \"compare\": head_raw,\n",
    "            \"commit_count\": 0,\n",
    "            \"commits\": []\n",
    "        }\n",
    "        write_json(out_path, payload)\n",
    "        errors += 1\n",
    "\n",
    "print(f\"Done. Written: {written} | Skipped: {skipped} | Errors (minimal outputs created): {errors}\")\n",
    "rate_limit_snapshot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
