{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd4d30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo root: /Users/rambodparsi/Desktop/OSS Repository Selection/web_scrapper/commit_pr_issue_analysis\n",
      "Bootstrap ready ✓\n"
     ]
    }
   ],
   "source": [
    "# === Bootstrap cell: shared config, paths, JSON I/O, and GitHub REST/GraphQL helpers ===\n",
    "\n",
    "import os, json, time, re, random, pathlib\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Locate repo root reliably (works no matter where you open the .ipynb)\n",
    "# -------------------------------------------------------------------\n",
    "def _find_repo_root(start: pathlib.Path) -> pathlib.Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(8):  # walk up to 8 levels\n",
    "        if (cur / \"config\" / \"config.yaml\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"Couldn't locate repo root (no config/config.yaml found upward).\")\n",
    "\n",
    "REPO_ROOT = _find_repo_root(pathlib.Path.cwd())\n",
    "CONFIG_PATH = REPO_ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load config + token\n",
    "# -------------------------------------------------------------------\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG: Dict[str, Any] = yaml.safe_load(f) or {}\n",
    "\n",
    "load_dotenv(REPO_ROOT / \".env\")  # local only; do not commit .env\n",
    "TOKEN_ENV = (CFG.get(\"github_token_env\") or \"GITHUB_TOKEN\").strip()\n",
    "GITHUB_TOKEN = os.getenv(TOKEN_ENV, \"\").strip()\n",
    "if not GITHUB_TOKEN:\n",
    "    raise RuntimeError(f\"Missing token in environment variable {TOKEN_ENV}. \"\n",
    "                       f\"Create .env from .env.example and set {TOKEN_ENV}=...\")\n",
    "\n",
    "# Repo + output root\n",
    "REPO = CFG[\"repo\"]  # \"owner/name\"\n",
    "OWNER, NAME = REPO.split(\"/\", 1)\n",
    "OUT_ROOT = pathlib.Path(CFG.get(\"out_root\") or NAME)  # \"<name>\" if null\n",
    "\n",
    "# Behavior\n",
    "OVERWRITE = bool(CFG.get(\"overwrite\", True))\n",
    "VERBOSE = bool(CFG.get(\"verbose_logs\", True))\n",
    "\n",
    "# Networking knobs\n",
    "REQ_TIMEOUT = int(CFG.get(\"request_timeout_sec\", 30))\n",
    "MAX_RETRIES = int(CFG.get(\"max_retries\", 4))\n",
    "BACKOFF_BASE_MS = int(CFG.get(\"backoff_base_ms\", 400))\n",
    "BACKOFF_JITTER_MS = int(CFG.get(\"backoff_jitter_ms\", 250))\n",
    "RESPECT_RL = bool(CFG.get(\"respect_rate_limits\", True))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Tiny logging helpers\n",
    "# -------------------------------------------------------------------\n",
    "def log(msg: str) -> None:\n",
    "    if VERBOSE:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def warn(msg: str) -> None:\n",
    "    print(f\"⚠️  {msg}\", flush=True)\n",
    "\n",
    "print(\"Using repo root:\", REPO_ROOT)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Canonical output layout helpers\n",
    "# -------------------------------------------------------------------\n",
    "def ensure_dir(p: pathlib.Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def repo_root() -> pathlib.Path:\n",
    "    root = OUT_ROOT\n",
    "    ensure_dir(root)\n",
    "    return root\n",
    "\n",
    "def tags_all_json() -> pathlib.Path:\n",
    "    d = repo_root() / \"tags\"\n",
    "    ensure_dir(d)\n",
    "    return d / \"tags.all.json\"\n",
    "\n",
    "def series_dir(kind: str, series: str) -> pathlib.Path:\n",
    "    d = repo_root() / kind / series\n",
    "    ensure_dir(d)\n",
    "    return d\n",
    "\n",
    "def pair_stem(base: str, compare: str) -> str:\n",
    "    return f\"{base}...{compare}\"\n",
    "\n",
    "def pair_json(series: str, stem: str, kind: str) -> pathlib.Path:\n",
    "    # kind ∈ {\"compare\",\"commits\",\"pulls\",\"issues\"}\n",
    "    return series_dir(kind, series) / f\"{stem}.{kind}.json\"\n",
    "\n",
    "def capsule_json(series: str, stem: str) -> pathlib.Path:\n",
    "    return series_dir(\"commits_pr_issue\", series) / f\"{stem}.tarce_artifacts.json\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# JSON I/O\n",
    "# -------------------------------------------------------------------\n",
    "def read_json(path: pathlib.Path) -> Optional[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(path: pathlib.Path, payload: Dict[str, Any]) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    if path.exists() and not OVERWRITE:\n",
    "        log(f\"Skip (exists): {path}\")\n",
    "        return\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    tmp.replace(path)\n",
    "    log(f\"✓ Wrote {path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GitHub HTTP session + rate-limit aware helpers\n",
    "# -------------------------------------------------------------------\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    \"User-Agent\": f\"notebook-pipeline/{NAME}\"\n",
    "})\n",
    "\n",
    "def _parse_reset_epoch(headers: Dict[str, Any]) -> Optional[int]:\n",
    "    try:\n",
    "        return int(headers.get(\"X-RateLimit-Reset\") or headers.get(\"x-ratelimit-reset\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _maybe_sleep_for_reset(resp: requests.Response) -> None:\n",
    "    if not RESPECT_RL:\n",
    "        return\n",
    "    remaining = resp.headers.get(\"X-RateLimit-Remaining\") or resp.headers.get(\"x-ratelimit-remaining\")\n",
    "    if remaining is not None and str(remaining).isdigit() and int(remaining) <= 0:\n",
    "        reset_epoch = _parse_reset_epoch(resp.headers)\n",
    "        if reset_epoch:\n",
    "            now = int(time.time())\n",
    "            delta = max(0, reset_epoch - now) + 1\n",
    "            warn(f\"Rate limit reached. Sleeping ~{delta}s until reset …\")\n",
    "            time.sleep(delta)\n",
    "\n",
    "def _backoff_sleep(i: int) -> None:\n",
    "    base = BACKOFF_BASE_MS / 1000.0\n",
    "    jitter = random.uniform(0, BACKOFF_JITTER_MS / 1000.0)\n",
    "    time.sleep((2 ** i) * base + jitter)\n",
    "\n",
    "def rest_get_json(url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generic GET with backoff + rate-limit handling.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for i in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = SESSION.get(url, params=params, timeout=REQ_TIMEOUT)\n",
    "            # Honor 429 Retry-After if present\n",
    "            if r.status_code == 429:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                if ra and ra.isdigit():\n",
    "                    wait_s = int(ra)\n",
    "                    warn(f\"429 received. Sleeping {wait_s}s per Retry-After …\")\n",
    "                    time.sleep(wait_s)\n",
    "                    continue\n",
    "            if r.status_code >= 500:\n",
    "                last_err = f\"{r.status_code} {r.text[:200]}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"HTTP {r.status_code}: {r.text[:500]}\")\n",
    "            _maybe_sleep_for_reset(r)\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            _backoff_sleep(i)\n",
    "    raise RuntimeError(f\"GET failed after retries: {url} :: {last_err}\")\n",
    "\n",
    "def gh_graphql(query: str, variables: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GitHub GraphQL POST with backoff + rate-limit handling.\n",
    "    Endpoint: https://api.github.com/graphql\n",
    "    \"\"\"\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    payload = {\"query\": query, \"variables\": variables or {}}\n",
    "    last_err = None\n",
    "    for i in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = SESSION.post(url, json=payload, timeout=REQ_TIMEOUT)\n",
    "            if r.status_code == 429:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                if ra and ra.isdigit():\n",
    "                    wait_s = int(ra)\n",
    "                    warn(f\"429 received. Sleeping {wait_s}s per Retry-After …\")\n",
    "                    time.sleep(wait_s)\n",
    "                    continue\n",
    "            if r.status_code >= 500:\n",
    "                last_err = f\"{r.status_code} {r.text[:200]}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"GraphQL HTTP {r.status_code}: {r.text[:500]}\")\n",
    "            _maybe_sleep_for_reset(r)\n",
    "            data = r.json()\n",
    "            if \"errors\" in data:\n",
    "                last_err = f\"GraphQL errors: {data['errors']}\"\n",
    "                _backoff_sleep(i)\n",
    "                continue\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            _backoff_sleep(i)\n",
    "    raise RuntimeError(f\"GraphQL failed after retries: {last_err}\")\n",
    "\n",
    "def rate_limit_snapshot() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns current REST rate-limit bucket (printed; not written to JSON outputs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info = rest_get_json(\"https://api.github.com/rate_limit\")\n",
    "        core = info.get(\"resources\", {}).get(\"core\", {})\n",
    "        remaining = core.get(\"remaining\")\n",
    "        limit = core.get(\"limit\")\n",
    "        reset = core.get(\"reset\")\n",
    "        when = datetime.fromtimestamp(reset, tz=timezone.utc).isoformat() if reset else None\n",
    "        snap = {\"limit\": limit, \"remaining\": remaining, \"reset_epoch\": reset, \"reset_iso\": when}\n",
    "        log(f\"Rate limit: {remaining}/{limit}, resets at {when}\")\n",
    "        return snap\n",
    "    except Exception as e:\n",
    "        warn(f\"Rate limit snapshot failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Light helpers for series + ordering\n",
    "# -------------------------------------------------------------------\n",
    "def semver_series(tag_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract 'vX.Y' series from tags like 'v4.2.3', '4.2.0-rc.1', etc.\n",
    "    Falls back to 'v0.0' if not parseable.\n",
    "    \"\"\"\n",
    "    m = re.search(r'v?(\\d+)\\.(\\d+)', tag_name or \"\")\n",
    "    if not m:\n",
    "        return \"v0.0\"\n",
    "    return f\"v{int(m.group(1))}.{int(m.group(2))}\"\n",
    "\n",
    "def sorted_pairs_by_tag_time(pairs: List[Tuple[str, str]], tag_index: Dict[str, Dict[str, Any]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Sort (base, compare) by tag timestamps ascending when both are tags;\n",
    "    unknown timestamps sort last.\n",
    "    \"\"\"\n",
    "    def ts(tag: str) -> float:\n",
    "        rec = tag_index.get(tag)\n",
    "        if rec and rec.get(\"tag_timestamp\"):\n",
    "            try:\n",
    "                return datetime.fromisoformat(rec[\"tag_timestamp\"].replace(\"Z\", \"+00:00\")).timestamp()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return float(\"inf\")\n",
    "    return sorted(pairs, key=lambda bc: (ts(bc[0]), ts(bc[1])))\n",
    "\n",
    "log(\"Bootstrap ready ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce30a0",
   "metadata": {},
   "source": [
    "### Config echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea690bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: mastodon/mastodon\n",
      "Output root: mastodon\n",
      "Overwrite: True | Verbose: True\n",
      "Rate limit: 4981/5000, resets at 2025-10-26T14:16:07+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'limit': 5000,\n",
       " 'remaining': 4981,\n",
       " 'reset_epoch': 1761488167,\n",
       " 'reset_iso': '2025-10-26T14:16:07+00:00'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Repo:\", REPO)\n",
    "print(\"Output root:\", repo_root())\n",
    "print(\"Overwrite:\", OVERWRITE, \"| Verbose:\", VERBOSE)\n",
    "rate_limit_snapshot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cc4a3",
   "metadata": {},
   "source": [
    "### Helpers for PR lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88c48656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helpers (existing + new verifier bits) ---\n",
    "\n",
    "import re\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "PR_REGEX = re.compile(r\"\\(#(\\d+)\\)\")\n",
    "\n",
    "def get_commit_message(owner: str, name: str, sha: str) -> Tuple[str, str]:\n",
    "    url = f\"https://api.github.com/repos/{owner}/{name}/commits/{sha}\"\n",
    "    c = rest_get_json(url)\n",
    "    meta = c.get(\"commit\") or {}\n",
    "    msg = meta.get(\"message\") or \"\"\n",
    "    headline = msg.splitlines()[0] if msg else \"\"\n",
    "    return headline, msg\n",
    "# Docs: Get a commit (REST). :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "def prs_via_rest_associated(owner: str, name: str, sha: str) -> List[Dict[str, Any]]:\n",
    "    url = f\"https://api.github.com/repos/{owner}/{name}/commits/{sha}/pulls\"\n",
    "    arr = rest_get_json(url)\n",
    "    out = []\n",
    "    for pr in arr or []:\n",
    "        num = pr.get(\"number\")\n",
    "        if not num: \n",
    "            continue\n",
    "        out.append({\n",
    "            \"number\": int(num),\n",
    "            \"pr_link\": pr.get(\"html_url\"),\n",
    "            \"api_url\": pr.get(\"url\"),\n",
    "            \"sources\": [\"rest_associated\"]\n",
    "        })\n",
    "    return out\n",
    "# Docs: List pull requests associated with a commit. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "def prs_via_graphql_associated(owner: str, name: str, sha: str) -> List[Dict[str, Any]]:\n",
    "    query = \"\"\"\n",
    "    query($owner:String!, $name:String!, $oid:GitObjectID!, $after:String) {\n",
    "      repository(owner:$owner, name:$name) {\n",
    "        object(oid:$oid) {\n",
    "          ... on Commit {\n",
    "            associatedPullRequests(first: 100, after: $after) {\n",
    "              pageInfo { hasNextPage endCursor }\n",
    "              nodes { number url }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\"\"\"\n",
    "    vars = {\"owner\": owner, \"name\": name, \"oid\": sha, \"after\": None}\n",
    "    out = []\n",
    "    while True:\n",
    "        data = gh_graphql(query, vars)\n",
    "        repo = (data.get(\"data\") or {}).get(\"repository\") or {}\n",
    "        obj = repo.get(\"object\") or {}\n",
    "        apr = (obj.get(\"associatedPullRequests\") or {})\n",
    "        for n in apr.get(\"nodes\") or []:\n",
    "            num = n.get(\"number\")\n",
    "            if num:\n",
    "                out.append({\n",
    "                    \"number\": int(num),\n",
    "                    \"pr_link\": n.get(\"url\"),\n",
    "                    \"api_url\": f\"https://api.github.com/repos/{owner}/{name}/pulls/{num}\",\n",
    "                    \"sources\": [\"gql_associated\"]\n",
    "                })\n",
    "        pi = (apr.get(\"pageInfo\") or {})\n",
    "        if not pi.get(\"hasNextPage\"):\n",
    "            break\n",
    "        vars[\"after\"] = pi.get(\"endCursor\")\n",
    "    return out\n",
    "# Docs: GraphQL Commit.associatedPullRequests. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "def pr_refs_from_message(message: str) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for m in PR_REGEX.finditer(message or \"\"):\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "            out.append({\n",
    "                \"number\": n,\n",
    "                \"pr_link\": f\"https://github.com/{OWNER}/{NAME}/pull/{n}\",\n",
    "                \"api_url\": f\"https://api.github.com/repos/{OWNER}/{NAME}/pulls/{n}\",\n",
    "                \"sources\": [\"message_ref\"]\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def merge_pr_refs(*lists: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen = set()\n",
    "    merged = []\n",
    "    for L in lists:\n",
    "        for r in L or []:\n",
    "            n = r.get(\"number\")\n",
    "            if n in seen:\n",
    "                for m in merged:\n",
    "                    if m.get(\"number\") == n:\n",
    "                        have = set(m.get(\"sources\") or [])\n",
    "                        for s in (r.get(\"sources\") or []):\n",
    "                            if s not in have:\n",
    "                                m[\"sources\"].append(s)\n",
    "                        break\n",
    "            else:\n",
    "                seen.add(n)\n",
    "                merged.append({**r, \"sources\": list(r.get(\"sources\") or [])})\n",
    "    return merged\n",
    "\n",
    "def is_bump_like_title(title: str) -> bool:\n",
    "    if not title:\n",
    "        return False\n",
    "    t = title.lower()\n",
    "    if any(k in t for k in [\"bump\", \"release\", \"version\"]):\n",
    "        return bool(re.search(r'v?\\d+\\.\\d+(\\.\\d+)?', t))\n",
    "    return False\n",
    "\n",
    "# ---------- NEW: verification helpers ----------\n",
    "def pr_commits_contains_sha(owner: str, name: str, pr_number: int, sha: str) -> bool:\n",
    "    \"\"\"\n",
    "    GET /repos/{owner}/{repo}/pulls/{number}/commits (caps at ~250). If 'sha' is in the list → verified.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{name}/pulls/{pr_number}/commits\"\n",
    "    try:\n",
    "        commits = rest_get_json(url)  # API returns an array\n",
    "        return any((c.get(\"sha\") == sha) for c in (commits or []))\n",
    "    except Exception as e:\n",
    "        warn(f\"Verifier pr_commits failed for PR #{pr_number}: {e}\")\n",
    "        return False\n",
    "# Docs: List commits on a pull request; note 250-item cap. :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "def pr_merge_commit_in_compare(owner: str, name: str, pr_number: int, compare_shas: set) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"\n",
    "    GET the PR to read merge_commit_sha; if that sha is in the compare set, treat as verified (squash/merge).\n",
    "    Returns (verified, merge_sha_or_None).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{name}/pulls/{pr_number}\"\n",
    "    try:\n",
    "        pr = rest_get_json(url)\n",
    "        msha = pr.get(\"merge_commit_sha\")\n",
    "        if msha and (msha in compare_shas):\n",
    "            return True, msha\n",
    "        return False, msha\n",
    "    except Exception as e:\n",
    "        warn(f\"Verifier merge_commit_sha failed for PR #{pr_number}: {e}\")\n",
    "        return False, None\n",
    "# Docs: merge_commit_sha semantics before/after merge. :contentReference[oaicite:6]{index=6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452b3c0",
   "metadata": {},
   "source": [
    "### Load planned pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "231c7ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO: mastodon/mastodon\n",
      "repo_root(): mastodon\n",
      "looking in: mastodon/compare\n",
      "found BEFORE whitelist: 107\n",
      "series present on disk: ['v4.0', 'v4.1', 'v4.2', 'v4.3', 'v4.4']\n",
      "applied whitelist ['v4.0'] → 19/107 remain\n",
      "sample:\n",
      " - mastodon/compare/v4.0/v4.0.0...v4.0.1.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.0rc1...v4.0.0rc2.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.0rc2...v4.0.0rc3.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.0rc3...v4.0.0rc4.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.0rc4...v4.0.0.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.1...v4.0.2.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.10...v4.0.11.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.11...v4.0.12.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.12...v4.0.13.compare.json\n",
      " - mastodon/compare/v4.0/v4.0.13...v4.0.14.compare.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"REPO:\", REPO)\n",
    "print(\"repo_root():\", repo_root())\n",
    "\n",
    "pairs_base = Path(repo_root()) / \"compare\"\n",
    "print(\"looking in:\", pairs_base)\n",
    "\n",
    "pairs_files = []\n",
    "if pairs_base.exists():\n",
    "    pairs_files = sorted(pairs_base.rglob(\"*.compare.json\"))\n",
    "\n",
    "# Broad fallback if someone ran with a different out_root previously\n",
    "if not pairs_files:\n",
    "    warn(\"No files under <name>/compare. Falling back to a broad search for */compare/**/*.compare.json …\")\n",
    "    pairs_files = sorted(Path(REPO_ROOT).rglob(\"compare/**/*.compare.json\"))\n",
    "\n",
    "print(\"found BEFORE whitelist:\", len(pairs_files))\n",
    "found_series = sorted({p.parent.name for p in pairs_files})\n",
    "print(\"series present on disk:\", found_series)\n",
    "\n",
    "SERIES_WHITELIST = CFG.get(\"series_whitelist\") or []\n",
    "if SERIES_WHITELIST:\n",
    "    before = len(pairs_files)\n",
    "    pairs_files = [p for p in pairs_files if p.parent.name in set(SERIES_WHITELIST)]\n",
    "    after = len(pairs_files)\n",
    "    print(f\"applied whitelist {SERIES_WHITELIST} → {after}/{before} remain\")\n",
    "\n",
    "print(\"sample:\")\n",
    "for p in pairs_files[:10]:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f271",
   "metadata": {},
   "source": [
    "### build the *.commits.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4284041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote mastodon/commits/v4.0/v4.0.0...v4.0.1.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.0rc1...v4.0.0rc2.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.0rc2...v4.0.0rc3.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.0rc3...v4.0.0rc4.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.0rc4...v4.0.0.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.1...v4.0.2.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.10...v4.0.11.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.11...v4.0.12.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.12...v4.0.13.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.13...v4.0.14.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.14...v4.0.15.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.2...v4.0.3.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.3...v4.0.4.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.4...v4.0.5.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.5...v4.0.6.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.6...v4.0.7.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.7...v4.0.8.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.8...v4.0.9.commits.json\n",
      "✓ Wrote mastodon/commits/v4.0/v4.0.9...v4.0.10.commits.json\n",
      "Done. Written: 19 | Skipped: 0\n",
      "Rate limit: 3791/5000, resets at 2025-10-26T14:16:07+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'limit': 5000,\n",
       " 'remaining': 3791,\n",
       " 'reset_epoch': 1761488167,\n",
       " 'reset_iso': '2025-10-26T14:16:07+00:00'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "written = 0\n",
    "skipped = 0\n",
    "\n",
    "for cmp_path in pairs_files:\n",
    "    pair_payload = read_json(cmp_path)\n",
    "    if not pair_payload:\n",
    "        continue\n",
    "\n",
    "    series = cmp_path.parent.name\n",
    "    base = pair_payload.get(\"base\")\n",
    "    comp = pair_payload.get(\"compare\")\n",
    "    stem = pair_stem(base, comp)\n",
    "    out_path = pair_json(series, stem, \"commits\")\n",
    "\n",
    "    if out_path.exists() and not OVERWRITE:\n",
    "        log(f\"Skip (exists): {out_path}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    compare_commits = pair_payload.get(\"commits\") or []\n",
    "    compare_sha_set = {c.get(\"sha\") for c in compare_commits if c.get(\"sha\")}\n",
    "\n",
    "    items = []\n",
    "\n",
    "    # Pass 1 — collect raw refs and titles; build PR frequency\n",
    "    pr_freq = {}\n",
    "    titles = {}\n",
    "    raw_refs = {}\n",
    "\n",
    "    for c in compare_commits:\n",
    "        sha = c.get(\"sha\")\n",
    "        if not sha:\n",
    "            continue\n",
    "\n",
    "        title, full_msg = get_commit_message(OWNER, NAME, sha)\n",
    "        titles[sha] = title\n",
    "\n",
    "        msg_refs = pr_refs_from_message(full_msg)\n",
    "        rest_refs = prs_via_rest_associated(OWNER, NAME, sha)\n",
    "        gql_refs  = [] if rest_refs else prs_via_graphql_associated(OWNER, NAME, sha)\n",
    "\n",
    "        merged = merge_pr_refs(rest_refs, gql_refs, msg_refs)\n",
    "        raw_refs[sha] = merged\n",
    "\n",
    "        for r in merged:\n",
    "            n = r.get(\"number\")\n",
    "            if n is not None:\n",
    "                pr_freq[n] = pr_freq.get(n, 0) + 1\n",
    "\n",
    "    # Detect “release-like” PRs that show up on many commits\n",
    "    many_threshold = max(5, int(0.2 * max(1, len(compare_commits))))\n",
    "    release_like_prs = {n for (n, f) in pr_freq.items() if f >= many_threshold}\n",
    "\n",
    "    # Pass 2 — verify and suppress as configured\n",
    "    for c in compare_commits:\n",
    "        sha = c.get(\"sha\")\n",
    "        title = titles.get(sha, \"\")\n",
    "        refs = raw_refs.get(sha, [])\n",
    "\n",
    "        # First, optional verification for each PR ref\n",
    "        verified_refs = []\n",
    "        for r in refs:\n",
    "            n = r.get(\"number\")\n",
    "            meta = {\"verified\": False, \"verification_method\": \"none\"}\n",
    "            if isinstance(n, int):\n",
    "                # 1) check pr_commits\n",
    "                if pr_commits_contains_sha(OWNER, NAME, n, sha):\n",
    "                    meta = {\"verified\": True, \"verification_method\": \"pr_commits\"}\n",
    "                else:\n",
    "                    # 2) check merge_commit_sha in compare range (squash/merge)\n",
    "                    ok, msha = pr_merge_commit_in_compare(OWNER, NAME, n, compare_sha_set)\n",
    "                    if ok:\n",
    "                        meta = {\"verified\": True, \"verification_method\": \"merge_commit_in_range\"}\n",
    "                r = {**r, \"meta\": meta}\n",
    "            else:\n",
    "                r = {**r, \"meta\": meta}\n",
    "            verified_refs.append(r)\n",
    "\n",
    "        # Apply the release-like suppression (unchanged)\n",
    "        if is_bump_like_title(title):\n",
    "            keep = verified_refs\n",
    "        else:\n",
    "            keep = [r for r in verified_refs if r.get(\"number\") not in release_like_prs]\n",
    "\n",
    "        items.append({\n",
    "            \"sha\": sha,\n",
    "            \"commit_title\": title,\n",
    "            \"pr_refs\": keep\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"repo\": REPO,\n",
    "        \"base\": base,\n",
    "        \"compare\": comp,\n",
    "        \"series\": series,\n",
    "        \"items\": items,\n",
    "        \"pair_annotations\": {\n",
    "            \"release_like_prs\": sorted(list(release_like_prs)),\n",
    "            \"release_like_rule\": {\"min_hits\": many_threshold, \"percent_of_commits\": 0.2}\n",
    "        }\n",
    "    }\n",
    "    write_json(out_path, payload)\n",
    "\n",
    "    written += 1\n",
    "\n",
    "print(f\"Done. Written: {written} | Skipped: {skipped}\")\n",
    "rate_limit_snapshot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
